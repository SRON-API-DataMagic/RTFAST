{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e1543b2",
   "metadata": {},
   "source": [
    "# Assorted sampling\n",
    "This notebook is intended to give users templates of using different samplers as a way to fit their code. It is assumed that you have already read/used emulator_demo before reading this notebook. This notebook is fairly minimal on explanation but each section gives you what is needed to use a particular sampling technique. We cover 4 different sampling techniques here but it should be acknowledged that there are even more techniques out there as well as packages to implement them which should be somewhat easy to implement as long as you know how to call RTFAST and convolve it with a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad1c539",
   "metadata": {},
   "source": [
    "## Nested sampling (dynesty)\n",
    "This is the technique used in emulator_demo. This section is largely a repeat of that seen in emulator_demo.\n",
    "\n",
    "We can also explore our parameter space with nested sampling. You can find out more about dynesty and how nested sampling works here: https://dynesty.readthedocs.io/en/stable/. What we care about is how it can help us map the posterior of our fairly complex and degenerate model.\n",
    "\n",
    "Nested sampling boasts some nice gains, prinicipally being able to derive statistical uncertainties as well as being able to sample from complex multimodal distributions. It also tends to be quite fast in comparison to emcee as well as being able to retrieve the Bayesian evidence - key for model comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f60cb5",
   "metadata": {},
   "source": [
    "## MCMC (Emcee)\n",
    "X-ray astronomers tend to already be familiar with emcee - they already use it in the various X-ray fitting software available. Rtdist has been used with emcee before, albeit it converges on the order of months. This obviously makes it practically extremely difficult to use this in actual fitting. Emcee doesn't exactly take no time to converge for the emulator either, but this is reduced to the order of hours on a standard laptop. This makes investigation with emcee much more achievable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a0510",
   "metadata": {},
   "source": [
    "Before we actually run our emcee, we need to establish a basic starting point for our emcee to start. For this, we'll use scipy's minimize optimizer to find a best starting point so that our walkers don't end up getting stuck in low-probability space. We'll then perturb that best starting point to form a Gaussian ball around our best starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify number of dimensions (parameters) and number of walkers\n",
    "ndim, nwalkers = 20, 100\n",
    "\n",
    "nll = lambda *args: -log_likelihood(*args)\n",
    "p0 = np.random.rand(ndim)\n",
    "initial = (p0*(limits[:,1]-limits[:,0])) + limits[:,0]\n",
    "soln = minimize(nll, np.asarray(nn_pars))\n",
    "\n",
    "start_pos = soln.x[:,np.newaxis] + np.random.randn(ndim,nwalkers)*0.01\n",
    "start_pos = start_pos.T\n",
    "\n",
    "#create emcee sampler\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_likelihood)\n",
    "print(\"Sampler started\")\n",
    "max_n = 50000 # maximum iterations, set this larger if not converging\n",
    "\n",
    "# We'll track how the average autocorrelation time estimate changes\n",
    "index = 0\n",
    "autocorr = np.empty(max_n)\n",
    "\n",
    "# This will be useful to testing convergence\n",
    "old_tau = np.inf\n",
    "\n",
    "# Now we'll sample for up to max_n steps\n",
    "for sample in sampler.sample(start_pos, iterations=max_n, progress=True):\n",
    "    # Only check convergence every 500 steps\n",
    "    if sampler.iteration % 500:\n",
    "        continue\n",
    "\n",
    "    # Compute the autocorrelation time so far\n",
    "    # Using tol=0 means that we'll always get an estimate even\n",
    "    # if it isn't trustworthy\n",
    "    tau = sampler.get_autocorr_time(tol=0)\n",
    "    autocorr[index] = np.mean(tau)\n",
    "    index += 1\n",
    "\n",
    "    # Check convergence\n",
    "    converged = np.all(tau * 100 < sampler.iteration)\n",
    "    converged &= np.all(np.abs(old_tau - tau) / tau < 0.01)\n",
    "    if converged:\n",
    "        print(\"Converged\")\n",
    "        break\n",
    "    old_tau = tau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68993ca",
   "metadata": {},
   "source": [
    "We should check the acceptance fraction as well as see if we've actually converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedbfd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Mean acceptance fraction: {0:.3f}\".format(\n",
    "        np.mean(sampler.acceptance_fraction)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Mean autocorrelation time: {0:.3f} steps\".format(\n",
    "        np.mean(sampler.get_autocorr_time())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a303d36",
   "metadata": {},
   "source": [
    "As a letter of caution, rtdist tends to have a low acceptance fraction so make sure you actually converge correctly. Let's plot our chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a2cb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(labels), figsize=(10, len(labels)*3), sharex=True)\n",
    "samples = sampler.get_chain()\n",
    "true_pars = np.asarray(nn_pars)\n",
    "for i in range(ndim):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "    ax.axhline(nn_pars[i],ls=\"--\",c=\"b\")\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea464cf3",
   "metadata": {},
   "source": [
    "Let's throw away 1000 steps to be safe, and thin our posterior by a factor of 5 and plot the posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7794ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_samples = sampler.get_chain(discard=10000,thin=3,flat=True)\n",
    "figure = corner.corner(\n",
    "    flat_samples,\n",
    "    labels=labels,show_titles=True,\n",
    "    truths = np.asarray(nn_pars),\n",
    "    title_kwargs={\"fontsize\": 12},\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3165c436",
   "metadata": {},
   "source": [
    "## Hamilton Monte Carlo (Pyro)\n",
    "Let's say you're bored of waiting around for a few hours for your fits, or maybe you want to fit multiple objects simultaneously. Your parameter space is suddenly considerably larger and techniques like MCMC and nested sampling begin to struggle to sample your parameter space effectively and the time to converge becomes even LONGER. We can use a variation of Monte Carlo samplers called a Hamilton Monte Carlo (HMC) sampler.\n",
    "\n",
    "HMCs have a key difference: they use gradients of your parameter space in respect to the likelihood to better sample the posterior and increase the acceptance rate of your evaluations. There are also several implementations of HMCs that are GPU compatible in Python. As our neural network is GPU compatible, we can load it onto the GPU and perform our fitting on GPUs which greatly speeds up model evaluation as well as achieving effortless parallelisation.\n",
    "\n",
    "Thus, we achieve two things: parallelisation of sample evaluations meaning we can increase the number of walkers in our sampler considerably (leading to a better coverage of the posterior) as well as speeding up both our evaluations and needing to perform fewer evaluations to converge.\n",
    "\n",
    "HMCs scale effectively into higher dimensions and allow us to start building hierarchical models that allow us to constrain population statistics and other things that every source should \"know\" about each other. This opens up a lot of interesting new avenues of research that we couldn't really access by hacking x-ray fitting software such as Xspec.\n",
    "\n",
    "We'll use the package called Pyro in this notebook as it is based off pytorch that our emulator is built on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from torch.distributions import constraints\n",
    "from pyro.infer import MCMC, NUTS\n",
    "from network import RTFAST\n",
    "from pyro.nn import PyroModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cd7f94",
   "metadata": {},
   "source": [
    "Let's write a function that uses pyro's structure to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emulator = SpectralEmulator()\n",
    "pyro_emulator = PyroModule(emulator)\n",
    "\n",
    "def pyro_model(data):\n",
    "    h   = pyro.sample(\"h\",   dist.Uniform(np.log10(1.5), np.log10(10)))\n",
    "    a   = pyro.sample(\"a\",   dist.Uniform(0.5, 0.998))\n",
    "    inc = pyro.sample(\"inc\", dist.Uniform(np.log10(30), np.log10(80)))\n",
    "    rin = pyro.sample(\"rin\", dist.Uniform(np.log10(1), np.log10(20)))\n",
    "    gam = pyro.sample(\"gam\", dist.Uniform(2, 2.75))\n",
    "    dis = pyro.sample(\"dis\", dist.Uniform(np.log10(3.5e5), np.log10(5e7)))\n",
    "    afe = pyro.sample(\"afe\", dist.Uniform(np.log10(0.5), np.log10(3)))\n",
    "    lNe = pyro.sample(\"lNe\", dist.Uniform(15,20))\n",
    "    nH  = pyro.sample(\"nH\",  dist.Uniform(np.log10(1e-3), np.log10(1)))\n",
    "    Ano = pyro.sample(\"Ano\", dist.Uniform(np.log10(1e-4),np.log10(1e-1)))\n",
    "    \n",
    "    rout = torch.log10(torch.Tensor([2e4]))[0].double()\n",
    "    z = torch.Tensor([0.024917])[0].double()\n",
    "    kte = torch.log10(torch.Tensor([50]))[0].double()\n",
    "    bst = torch.log10(torch.Tensor([1]))[0].double()\n",
    "    mas = torch.log10(torch.Tensor([3e6]))[0].double()\n",
    "    hor = torch.Tensor([0.02])[0].double()\n",
    "    b1 = torch.Tensor([0])[0].double()\n",
    "    b2 = torch.Tensor([0])[0].double()\n",
    "    pAB = torch.Tensor([-0.8])[0].double()\n",
    "    g = torch.Tensor([0.3])[0].double()\n",
    "    # Stack parameters to create the input to the neural network\n",
    "    params = torch.stack([h,a,inc,rin,rout,z,gam,dis,afe,lNe,kte,nH,bst,mas,hor,b1,b2,pAB,g,Ano])\n",
    "    \n",
    "    #call emulator and convolve with instrument response\n",
    "    pred = pyro_emulator(params)\n",
    "    convolved = torch.matmul(pred,resp.resp_matrix.double())\n",
    "    pred = torch.transpose(convolved,0,-1)\n",
    "    \n",
    "    pred = torch.max(pred, torch.tensor(1e-11))  # Add a small epsilon to avoid zeros\n",
    "    \n",
    "    with pyro.plate(\"data\", len(data)):\n",
    "        pyro.sample(\"obs\", dist.Poisson(pred), obs=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ecf30",
   "metadata": {},
   "source": [
    "Now that we've defined our model in pyro, let's run a NUTS sampler to retrieve our posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679d1be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_kernel = NUTS(pyro_model)\n",
    "\n",
    "# Set up MCMC with the NUTS sampler\n",
    "mcmc = MCMC(nuts_kernel, num_samples=10, warmup_steps=400, num_chains=1,)\n",
    "\n",
    "# Run MCMC to sample from the posterior\n",
    "mcmc.run(torch.Tensor(pois_obs))\n",
    "\n",
    "# Get samples from the posterior\n",
    "posterior_samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c11f81",
   "metadata": {},
   "source": [
    "Let's check if the sampler has converged by calculating the Gelman-Rubin statistic. Our sampler has converged if our GR is less than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0007f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "# Convert Pyro samples to InferenceData\n",
    "inference_data = az.from_pyro(mcmc)\n",
    "# Calculate Gelman-Rubin statistic (PSRF)\n",
    "rhat = az.rhat(inference_data)\n",
    "print(rhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb592d",
   "metadata": {},
   "source": [
    "Once we're happy that the sampler has converged, we can plot our posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad03360",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = corner.corner(posterior_samples,show_titles=True,\n",
    "    truths = np.asarray(nn_pars_hmc),\n",
    "    title_kwargs={\"fontsize\": 12},\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675deca1",
   "metadata": {},
   "source": [
    "Finally, let's plot draws from our posteriors to see if our model outputs are reasonable compared to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2418f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmc_samples = []\n",
    "names = []\n",
    "for name, samples in posterior_samples.items():\n",
    "    hmc_samples.append(samples)\n",
    "    names.append(name)\n",
    "hmc_samples = np.asarray(hmc_samples).T\n",
    "#The following line rearranges the pyro inference object in the same order of parameters as RTFAST inputs\n",
    "hmc_samples = hmc_samples[:,[5,1,6,-1,4,3,2,-3,-2,0]]\n",
    "\n",
    "inds = np.random.randint(len(hmc_samples), size=100)\n",
    "model_draws = []\n",
    "for ind in inds:\n",
    "    sample = hmc_samples[ind]\n",
    "    model_eval = convolve_sim_fixed(sample)\n",
    "    model_draws.append(model_eval)\n",
    "    plt.plot(emid, model_eval, \"C1\", alpha=0.1)\n",
    "\n",
    "model_draws = np.asarray(model_draws)\n",
    "plt.plot(emid,np.mean(model_draws,axis=0),\"r\",label=\"Mean model\")\n",
    "plt.plot(emid, convolved, \"k\", label=\"truth\",ls=\"--\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Energy/KeV\")\n",
    "plt.ylabel(\"Photons\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c830c4d7",
   "metadata": {},
   "source": [
    "## Simulation based inference (SBI)\n",
    "Yet another alternative fitting procedure is simulation based inference. Simulation based inference functions by training a neural network to learn what the posterior will look like for a simulator given a set of data. This method can be really fast, but it should be cautioned that there have been papers that have shown sbi to give optimistic posterior evaluations in comparison to MCMC: https://arxiv.org/abs/2110.06581. SBI also works a little differently than other traditional methods - by simulating the instrumental and underlying effects (Poissonian noise) of the source.\n",
    "\n",
    "Let's start by importing the necessary packages for using sbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d947df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sbi.analysis import pairplot\n",
    "from sbi.inference import SNPE, simulate_for_sbi\n",
    "from sbi.utils import BoxUniform\n",
    "from sbi.utils.user_input_checks import (\n",
    "    check_sbi_inputs,\n",
    "    process_prior,\n",
    "    process_simulator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a99f6c1",
   "metadata": {},
   "source": [
    "In this case, we'll use the convolved simulation function that we used in nested sampling as our simulator- as that simulates what our final data looks like. Let's define our priors in SBI compatible form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec54445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator_sbi(theta):\n",
    "    pred = convolve_sim_fixed(theta)\n",
    "    return np.random.poisson(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd4e670",
   "metadata": {},
   "source": [
    "We then need to redefine our priors using sbi's prior formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35785d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dim = 3\n",
    "\n",
    "height_range = [np.log10(1.5),np.log10(10)]\n",
    "spin_range = [0.5,0.998]\n",
    "inclination_range = [np.log10(30),np.log10(80)]\n",
    "r_inner_range = [np.log10(1),np.log10(20)]\n",
    "Gamma_range = [2,2.75]\n",
    "distance_range = [np.log10(3.5e5),np.log10(5e7)]\n",
    "Afe_range = [np.log10(0.5),np.log10(3)]\n",
    "logNe_range = [15,20]\n",
    "nH_range = [np.log10(1e-3),np.log10(1)]\n",
    "anorm_range = [np.log10(1e-4),np.log10(1e-1)]\n",
    "\n",
    "prior_ranges = [Gamma_range,nH_range,anorm_range]\n",
    "\n",
    "prior_ranges = np.asarray(prior_ranges)\n",
    "\n",
    "prior = BoxUniform(low=torch.Tensor(prior_ranges[:,0]), high=torch.Tensor(prior_ranges[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a88ba5b",
   "metadata": {},
   "source": [
    "We then need to simulate some observations for our network to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cbaa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior, num_parameters, prior_returns_numpy = process_prior(prior)\n",
    "simulator_sbi = process_simulator(simulator_sbi, prior, prior_returns_numpy)\n",
    "check_sbi_inputs(simulator_sbi, prior)\n",
    "\n",
    "inference = SNPE(prior)\n",
    "\n",
    "posteriors = []\n",
    "proposal = prior\n",
    "\n",
    "num_rounds = 2\n",
    "\n",
    "for _ in range(num_rounds):\n",
    "    print(_)\n",
    "    theta, x = simulate_for_sbi(simulator_sbi, proposal, num_simulations=10000)\n",
    "    density_estimator = inference.append_simulations(\n",
    "        theta, x, proposal=proposal\n",
    "    ).train()\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    posteriors.append(posterior)\n",
    "    proposal = posterior.set_default_x(pois_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f986172",
   "metadata": {},
   "source": [
    "Let's plot the resulting posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ab1c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for posterior in posteriors:\n",
    "    samples = posterior.sample((10000,), x=pois_obs)\n",
    "    _ = pairplot(samples,points=np.asarray([2.45,np.log10(5e-2),np.log10(4e-4)]),\n",
    "             labels=[r\"$\\Gamma$\",r\"$N_H$\",r\"$A_{norm}$\"],\n",
    "             quantiles=[0.03,99.7])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64720951",
   "metadata": {},
   "source": [
    "And plot some posterior draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0721ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = posteriors[0].sample((10000,), x=pois_obs)\n",
    "fig, axs = plt.subplots(2,sharex=True,figsize=(8,8))\n",
    "inds = np.random.randint(0,samples.shape[0],size=500)\n",
    "model_draws = []\n",
    "for ind in inds:\n",
    "    sample = samples[ind]\n",
    "    model_eval = convolve_sim_fixed(sample)\n",
    "    model_draws.append(model_eval)\n",
    "    axs[0].plot(emid, model_eval, \"C1\", alpha=0.1)\n",
    "\n",
    "model_draws = np.asarray(model_draws)\n",
    "axs[0].plot(emid,np.mean(model_draws,axis=0),\"r\",label=\"Mean model\")\n",
    "axs[0].plot(emid, pois_obs, \"k\", label=\"observation\",ls=\"--\")\n",
    "axs[0].legend(fontsize=14)\n",
    "fig.supxlabel(\"Energy/KeV\")\n",
    "axs[0].set_ylabel(\"Photons\")\n",
    "axs[0].set_xscale(\"log\")\n",
    "axs[0].set_yscale(\"log\")\n",
    "axs[0].set_ylim(1)\n",
    "axs[1].scatter(emid,(pois_obs-np.mean(model_draws,axis=0))/np.sqrt(pois_obs),s=1,marker=\"+\")\n",
    "axs[1].set_ylabel(\"(data-model)/data\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
